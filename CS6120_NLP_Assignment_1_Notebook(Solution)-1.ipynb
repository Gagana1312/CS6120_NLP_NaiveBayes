{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc97d6da",
   "metadata": {
    "id": "bc97d6da"
   },
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 1: Naive Bayes\n",
    "### Total Points: 100 points\n",
    "\n",
    "You will be dealing with movie review data that includes both positive and negative reviews in this assignment. You will use Sentiment Analysis to assess if a given review is positive or negative using the provided dataset.\n",
    "\n",
    "Therefore, we will make use of Naive Bayes algorithm to perform sentiment analysis on the movie review dataset.\n",
    "\n",
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03450ac",
   "metadata": {
    "id": "a03450ac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc584cc2",
   "metadata": {
    "id": "fc584cc2"
   },
   "source": [
    "## Reading the data\n",
    "\n",
    "When reading the data, ensure that the '.csv' file is in the same location where your jupyter notebook is used. This way the files are organized and easy to read using the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9ffbf5",
   "metadata": {
    "id": "3c9ffbf5"
   },
   "outputs": [],
   "source": [
    "## Reading the data and removing columns that are not important. \n",
    "df = pd.read_csv(\"/Users/gaganaananda/Downloads/movie_reviews-1.csv\", sep = ',', encoding = 'latin-1', usecols = lambda col: col not in [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fa8ac0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "f7fa8ac0",
    "outputId": "69edaf28-1e2e-4ec7-9530-b30a2853ab9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "4  Probably my all-time favorite movie, a story o...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # print head of data frame with help of head function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1749da04",
   "metadata": {
    "id": "1749da04"
   },
   "source": [
    "## Count plot of the output categories: positive or negative\n",
    "\n",
    "Feel free to take a look at the output and whether the classes are balanced or imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c152e8a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "c152e8a4",
    "outputId": "a2ba0476-3238-4511-fcf0-780508493f48"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "sns.countplot(x = df['sentiment'])\n",
    "plt.title(\"Positive Vs. Negative reviews count\", fontsize = 15)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b8549f2",
   "metadata": {
    "id": "9b8549f2"
   },
   "source": [
    "## Upsampling the minority class: (5 points)\n",
    "\n",
    "It is known that Naive bayes is not robust to class imbalance. It could be seen above that the data is little imbalanced. Therefore, class balancing can be done before giving it to the Naive Bayes model for prediction. \n",
    "\n",
    "Feel free to use 'resample' library from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yHJTAqrW7XMN",
   "metadata": {
    "id": "yHJTAqrW7XMN"
   },
   "outputs": [],
   "source": [
    "## hint: use resample from sklearn.utils\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df.sentiment=='positive']\n",
    "df_minority = df[df.sentiment=='negative']\n",
    "\n",
    "negative_upsample = resample(df_minority, replace = True, \n",
    "                        n_samples = df_majority.shape[0],\n",
    "                        random_state = 101)\n",
    "\n",
    "df_upsampled = pd.concat([df_majority,negative_upsample])  # concat two data frames i,e majority class data set and upsampled minority class data set\n",
    "df_upsampled = df_upsampled.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9329bb",
   "metadata": {
    "id": "6a9329bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12474, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Just to ensure that upsampling was done successfully, take a look at the shape of the data in \n",
    "## this cell. \n",
    "\n",
    "# print the shape of data set with the help of shape function having \"negative\" as class label\n",
    "df_upsampled[df_upsampled['sentiment']=='negative'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8bf6e7",
   "metadata": {
    "id": "6f8bf6e7"
   },
   "source": [
    "### Expected Output : \n",
    "(12474, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdea8155",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdea8155",
    "outputId": "c665c4b9-826e-4f4e-e30e-06e0935a0622"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12474, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ensure that the same number of data points are present for both 'positive' and 'negative' data\n",
    "\n",
    "# print the shape of data set with the help of shape function having \"positive\" as class label\n",
    "df_upsampled[df_upsampled['sentiment']=='positive'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "626f01d5",
   "metadata": {
    "id": "626f01d5"
   },
   "source": [
    "### Expected Output : \n",
    "(12474, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "NoW5z6SzAeP8",
   "metadata": {
    "id": "NoW5z6SzAeP8"
   },
   "outputs": [],
   "source": [
    "## In this cell, we are going to be dividing the data into train and test points\n",
    "## Ensure that you store the upsampled data in a variable called 'df_upsampled' \n",
    "## so that the below operations are performed successfully\n",
    "\n",
    "\n",
    "## Considering 10000 positive and 10000 negative data points\n",
    "negative_data_points_train = df_upsampled[df_upsampled['sentiment'] == 'negative'].iloc[:10000]\n",
    "positive_data_points_train = df_upsampled[df_upsampled['sentiment'] == 'positive'].iloc[:10000]\n",
    "\n",
    "## Considering the remaining data points for test\n",
    "negative_data_points_test = df_upsampled[df_upsampled['sentiment'] == 'negative'].iloc[10000:]\n",
    "positive_data_points_test = df_upsampled[df_upsampled['sentiment'] == 'positive'].iloc[10000:]\n",
    "\n",
    "## Concatenate the training positive and negative reviews\n",
    "X_train = pd.concat([positive_data_points_train['review'],negative_data_points_train['review']])\n",
    "## Concatenating the training positive and negative outputs\n",
    "y_train = pd.concat([positive_data_points_train['sentiment'], negative_data_points_train['sentiment']])\n",
    "\n",
    "## Concatenating the test positive and negative reviews\n",
    "X_test = pd.concat([positive_data_points_test['review'],negative_data_points_test['review']])\n",
    "## Concatenating the test positive and negative outputs\n",
    "y_test = pd.concat([positive_data_points_test['sentiment'], negative_data_points_test['sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6428047d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6428047d",
    "outputId": "10d10601-0ce0-4688-c4d3-75fa386583fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    10000\n",
       "negative    10000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take a look at the total number of classes and their count using '.value_counts()' for y_train and y_test.\n",
    "## Ensure that there are equal number of positive and negative reviews. \n",
    "y_train.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dfe6517",
   "metadata": {
    "id": "7dfe6517"
   },
   "source": [
    "### Expected Output:\n",
    "negative    10000<br>\n",
    "positive    10000<br>\n",
    "Name: sentiment, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2beae1d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2beae1d6",
    "outputId": "6896f930-6a1a-45db-b74c-d080c9aedd0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    2474\n",
       "negative    2474\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9163f897",
   "metadata": {
    "id": "9163f897"
   },
   "source": [
    "### Expected Output : \n",
    "negative    2474<br>\n",
    "positive    2474<br>\n",
    "Name: sentiment, dtype: int64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6501699b",
   "metadata": {
    "id": "6501699b"
   },
   "source": [
    "## Q1. Pre-process the reviews: (10 points)\n",
    "\n",
    "We know that a review contains links, punctuation, stopwords and many other words that don't give a lot of meaning for the Naive Bayes model for prediction. \n",
    "\n",
    "In the cell below, one must implement text-preprocessing and remove links, punctuations and stopwords. It is also important to lowercase the letters so that 'Admire' and 'admire' are not treated as different words. \n",
    "\n",
    "In addition to this, perform stemming operation so that similar words are reduced. To know more about stemming, feel free to take a look at this link.\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "CirLN9-ddQ1r",
   "metadata": {
    "id": "CirLN9-ddQ1r"
   },
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    '''\n",
    "    Input:\n",
    "        review: a string containing a review.\n",
    "    Output:\n",
    "        review_cleaned: a processed review. \n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    english_stopwords = stopwords.words('english') ## Removing stopwords\n",
    "    \n",
    "    rhtml = re.sub(r'<.*?>', '', review)\n",
    "    review = re.sub(r'http\\S+|www\\S+|https\\S+', '', rhtml, flags=re.MULTILINE) ## link\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    review = re.sub(r'$[a-zA-Z]*', '', review) # words\n",
    "    review_tokens = word_tokenize(review) ## tokenizing the review into words\n",
    "\n",
    "    review_cleaned = []\n",
    "    for word in review_tokens:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        if stem_word.lower() not in review_cleaned:\n",
    "            review_cleaned.append(stem_word.lower())\n",
    "\n",
    "    # Convert the list of cleaned words to a single string\n",
    "    #review_cleaned = ' '.join(review_cleaned)\n",
    "\n",
    "    return review_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7632fe5",
   "metadata": {
    "id": "a7632fe5"
   },
   "source": [
    "## Q2. Implement a find_occurrence function (5 points):\n",
    "\n",
    "In this function, we find the total occurrence of a word giving information such as label, word and frequency dictionary.\n",
    "\n",
    "Note that this function is used later in the code when we are going to be predicting the output using Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb282b81",
   "metadata": {
    "id": "eb282b81"
   },
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def find_occurrence(frequency, word, label):\n",
    "    '''\n",
    "    Params:\n",
    "        frequency: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Return:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = frequency.get((word, label), 0)\n",
    "  \n",
    "    return n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29a2249d",
   "metadata": {
    "id": "29a2249d"
   },
   "source": [
    "### Converting output to numerical format:\n",
    "\n",
    "We have outputs as 'positive' or 'negative'. In the cell below, we convert it to a numerical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdc2b2c",
   "metadata": {
    "id": "bcdc2b2c"
   },
   "outputs": [],
   "source": [
    "## With the use of mapping function, we replace\n",
    "## the label in the form of string to an integer. \n",
    "\n",
    "output_map = {'positive': 0, 'negative': 1}\n",
    "y_train = y_train.map(output_map)\n",
    "y_test = y_test.map(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dde0bbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dde0bbd",
    "outputId": "223dfbc1-8efe-4183-b6d7-c9cb025cb285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "1    10000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ensuring that there are equal number of classes on the training data. \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2959b85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "f2959b85",
    "outputId": "e514214b-cd57-43fc-875d-1821ddab63f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I saw this ages ago when I was younger and could never remember the title, until one day I was scrolling through John Candy\\'s film credits on IMDb and noticed an entry named \"Once Upon a Crime...\". Something rang a bell and I clicked on it, and after reading the plot summary it brought back a lot of memories.<br /><br />I\\'ve found it has aged pretty well despite the fact that it is not by any means a \"great\" comedy. It is, however, rather enjoyable and is a good riff on a Hitchcock formula of mistaken identity and worldwide thrills.<br /><br />The movie has a large cast of characters, amongst them an American couple who find a woman\\'s dog while vacationing in Europe and decide to return it to her for a reward - only to find her dead body upon arrival. From there the plot gets crazier and sillier and they go on the run after the police think they are the killers.<br /><br />Kind of a mix between \"It\\'s a Mad Mad Mad Mad World\" and a lighter Hitchcock feature, this was directed by Eugene Levy and he managed to get some of his good friends - such as John Candy - to star in it. The movie is mostly engaging due to its cast, and the ending has a funny little twist that isn\\'t totally unpredictable but also is kind of unexpected.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Choosing a random review and taking a look at it.\n",
    "X_train.iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed5e43c9",
   "metadata": {
    "id": "ed5e43c9"
   },
   "source": [
    "From the above cell output, it could be seen that there are a lot of words that don't add a lot of meaning to the text. \n",
    "\n",
    "Therefore, those words would be removed. It also reduces the computation time. \n",
    "\n",
    "Therefore, it is a good practice we are following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad3937ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad3937ea",
    "outputId": "68985efe-32cd-4c11-ed5b-2e1e1b297e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'saw', 'thi', 'age', 'ago', 'when', 'wa', 'younger', 'and', 'could', 'never', 'rememb', 'the', 'titl', 'until', 'one', 'day', 'scroll', 'through', 'john', 'candi', 'film', 'credit', 'on', 'imdb', 'notic', 'an', 'entri', 'name', 'onc', 'upon', 'a', 'crime', 'someth', 'rang', 'bell', 'click', 'it', 'after', 'read', 'plot', 'summari', 'brought', 'back', 'lot', 'of', 'memories', 'found', 'ha', 'pretti', 'well', 'despit', 'fact', 'that', 'is', 'not', 'by', 'ani', 'mean', 'great', 'comedi', 'howev', 'rather', 'enjoy', 'good', 'riff', 'hitchcock', 'formula', 'mistaken', 'ident', 'worldwid', 'thrillsth', 'movi', 'larg', 'cast', 'charact', 'amongst', 'them', 'american', 'coupl', 'who', 'find', 'woman', 'dog', 'while', 'vacat', 'in', 'europ', 'decid', 'to', 'return', 'her', 'for', 'reward', 'onli', 'dead', 'bodi', 'arriv', 'from', 'there', 'get', 'crazier', 'sillier', 'they', 'go', 'run', 'polic', 'think', 'are', 'killerskind', 'mix', 'between', 'mad', 'world', 'lighter', 'featur', 'direct', 'eugen', 'levi', 'he', 'manag', 'some', 'hi', 'friend', 'such', 'as', 'star', 'mostli', 'engag', 'due', 'end', 'funni', 'littl', 'twist', 'isnt', 'total', 'unpredict', 'but', 'also', 'kind', 'unexpect']\n"
     ]
    }
   ],
   "source": [
    "custom_review = X_train.iloc[0]\n",
    "\n",
    "# print cleaned review\n",
    "print(clean_review(custom_review))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e6cc440",
   "metadata": {
    "id": "3e6cc440"
   },
   "source": [
    "We now use this function to pre-process the review and remove words that don't add a lot of meaning in our model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a762960",
   "metadata": {
    "id": "5a762960"
   },
   "source": [
    "## Q3. Implementing review counter function: (5 points)\n",
    "\n",
    "It is now time to implement the count function for the reviews. \n",
    "\n",
    "In this function, we count the occurrence of words and get the probabilities \n",
    "for the words based on the training data. \n",
    "\n",
    "In other words, we get the probability of occurrence of a word, given that the output is 'positive'.\n",
    "\n",
    "Similarly, we also compute the probability of occurence of a word, given that the output is 'negative'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de61f77",
   "metadata": {
    "id": "5de61f77"
   },
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def review_counter(output_occurrence, reviews, positive_or_negative):\n",
    "    '''\n",
    "    Params:\n",
    "        output_occurrence: a dictionary that will be used to map each pair to its frequency\n",
    "        reviews: a list of reviews\n",
    "        positive_or_negative: a list corresponding to the sentiment of each review (either 0 or 1)\n",
    "    Return:\n",
    "        output: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ## Steps :\n",
    "    # define the key, which is the word and label tuple\n",
    "    # if the key exists in the dictionary, increment the count\n",
    "    # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "    \n",
    "    for label, review in zip(positive_or_negative, reviews):\n",
    "        split_review = clean_review(review)\n",
    "        for word in split_review:\n",
    "            kpair = (word, label)\n",
    "            if kpair in output_occurrence:\n",
    "                output_occurrence[kpair]+=1\n",
    "            else:\n",
    "                output_occurrence[kpair] = 1\n",
    "   \n",
    "    return output_occurrence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18238223",
   "metadata": {
    "id": "18238223"
   },
   "source": [
    "### Test your function with example reviews:\n",
    "\n",
    "Feel free to run the cell below and understand whether the above function that you have defined is producing the optimum results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07a4c58a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07a4c58a",
    "outputId": "dd9e148a-34a9-4cfe-9077-c5de33493d7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('got', 1): 1,\n",
       " ('bore', 1): 2,\n",
       " ('throught', 1): 1,\n",
       " ('the', 1): 1,\n",
       " ('moview', 1): 1,\n",
       " ('the', 0): 2,\n",
       " ('movi', 0): 2,\n",
       " ('wa', 0): 1,\n",
       " ('fantast', 0): 1,\n",
       " ('will', 1): 1,\n",
       " ('not', 1): 1,\n",
       " ('watch', 1): 1,\n",
       " ('it', 1): 2,\n",
       " ('again', 1): 1,\n",
       " ('wa', 1): 1,\n",
       " ('a', 1): 1,\n",
       " ('complet', 1): 1,\n",
       " ('wast', 1): 1,\n",
       " ('of', 1): 1,\n",
       " ('time', 1): 1,\n",
       " ('and', 1): 1,\n",
       " ('money', 1): 1,\n",
       " ('enjoy', 0): 1,\n",
       " ('to', 0): 1,\n",
       " ('fullest', 0): 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "result = {}\n",
    "reviews = ['got bored throught the moview', 'The movie was fantastic', 'Will not watch it again', 'Was bored, it was a complete waste of time and money', 'Enjoyed the movie to the fullest']\n",
    "ys = [1, 0, 1, 1, 0]\n",
    "review_counter(result,reviews, ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "927f89bb",
   "metadata": {
    "id": "927f89bb"
   },
   "source": [
    "### Expected Output:\n",
    " {('bored', 1): 2, <br>\n",
    " ('complete', 1): 1, <br>\n",
    " ('enjoyed', 0): 1, <br>\n",
    " ('fantastic', 0): 1, <br>\n",
    " ('fullest', 0): 1, <br>\n",
    " ('got', 1): 1, <br>\n",
    " ('money', 1): 1, <br>\n",
    " ('movie', 0): 2, <br>\n",
    " ('moview', 1): 1, <br>\n",
    " ('throught', 1): 1, <br>\n",
    " ('time', 1): 1, <br>\n",
    " ('waste', 1): 1, <br>\n",
    " ('watch', 1): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc62e13",
   "metadata": {
    "id": "9bc62e13"
   },
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "\n",
    "freqs = review_counter({}, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eddf420",
   "metadata": {
    "id": "0eddf420"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('i', 0): 7243,\n",
       " ('saw', 0): 1188,\n",
       " ('thi', 0): 8853,\n",
       " ('age', 0): 609,\n",
       " ('ago', 0): 412,\n",
       " ('when', 0): 3595,\n",
       " ('wa', 0): 6106,\n",
       " ('younger', 0): 210,\n",
       " ('and', 0): 9693,\n",
       " ('could', 0): 1852,\n",
       " ('never', 0): 1932,\n",
       " ('rememb', 0): 770,\n",
       " ('the', 0): 9886,\n",
       " ('titl', 0): 500,\n",
       " ('until', 0): 648,\n",
       " ('one', 0): 5665,\n",
       " ('day', 0): 1333,\n",
       " ('scroll', 0): 6,\n",
       " ('through', 0): 1417,\n",
       " ('john', 0): 724,\n",
       " ('candi', 0): 58,\n",
       " ('film', 0): 5981,\n",
       " ('credit', 0): 412,\n",
       " ('on', 0): 6179,\n",
       " ('imdb', 0): 180,\n",
       " ('notic', 0): 316,\n",
       " ('an', 0): 4954,\n",
       " ('entri', 0): 86,\n",
       " ('name', 0): 810,\n",
       " ('onc', 0): 803,\n",
       " ('upon', 0): 339,\n",
       " ('a', 0): 9675,\n",
       " ('crime', 0): 308,\n",
       " ('someth', 0): 1320,\n",
       " ('rang', 0): 145,\n",
       " ('bell', 0): 56,\n",
       " ('click', 0): 32,\n",
       " ('it', 0): 8965,\n",
       " ('after', 0): 2141,\n",
       " ('read', 0): 755,\n",
       " ('plot', 0): 1463,\n",
       " ('summari', 0): 65,\n",
       " ('brought', 0): 330,\n",
       " ('back', 0): 1472,\n",
       " ('lot', 0): 1549,\n",
       " ('of', 0): 9498,\n",
       " ('memories', 0): 1,\n",
       " ('found', 0): 922,\n",
       " ('ha', 0): 4196,\n",
       " ('pretti', 0): 954,\n",
       " ('well', 0): 3204,\n",
       " ('despit', 0): 470,\n",
       " ('fact', 0): 1119,\n",
       " ('that', 0): 7905,\n",
       " ('is', 0): 9060,\n",
       " ('not', 0): 5691,\n",
       " ('by', 0): 4700,\n",
       " ('ani', 0): 1911,\n",
       " ('mean', 0): 801,\n",
       " ('great', 0): 3320,\n",
       " ('comedi', 0): 1061,\n",
       " ('howev', 0): 1074,\n",
       " ('rather', 0): 842,\n",
       " ('enjoy', 0): 1701,\n",
       " ('good', 0): 3701,\n",
       " ('riff', 0): 14,\n",
       " ('hitchcock', 0): 92,\n",
       " ('formula', 0): 100,\n",
       " ('mistaken', 0): 46,\n",
       " ('ident', 0): 133,\n",
       " ('worldwid', 0): 21,\n",
       " ('thrillsth', 0): 1,\n",
       " ('movi', 0): 5882,\n",
       " ('larg', 0): 287,\n",
       " ('cast', 0): 1504,\n",
       " ('charact', 0): 3225,\n",
       " ('amongst', 0): 68,\n",
       " ('them', 0): 2149,\n",
       " ('american', 0): 779,\n",
       " ('coupl', 0): 589,\n",
       " ('who', 0): 4618,\n",
       " ('find', 0): 1753,\n",
       " ('woman', 0): 789,\n",
       " ('dog', 0): 248,\n",
       " ('while', 0): 1597,\n",
       " ('vacat', 0): 60,\n",
       " ('in', 0): 8778,\n",
       " ('europ', 0): 100,\n",
       " ('decid', 0): 541,\n",
       " ('to', 0): 9309,\n",
       " ('return', 0): 457,\n",
       " ('her', 0): 2621,\n",
       " ('for', 0): 7097,\n",
       " ('reward', 0): 98,\n",
       " ('onli', 0): 2816,\n",
       " ('dead', 0): 433,\n",
       " ('bodi', 0): 338,\n",
       " ('arriv', 0): 215,\n",
       " ('from', 0): 4653,\n",
       " ('there', 0): 3689,\n",
       " ('get', 0): 3280,\n",
       " ('crazier', 0): 7,\n",
       " ('sillier', 0): 6,\n",
       " ('they', 0): 3679,\n",
       " ('go', 0): 2412,\n",
       " ('run', 0): 783,\n",
       " ('polic', 0): 316,\n",
       " ('think', 0): 2416,\n",
       " ('are', 0): 5469,\n",
       " ('killerskind', 0): 1,\n",
       " ('mix', 0): 299,\n",
       " ('between', 0): 1226,\n",
       " ('mad', 0): 211,\n",
       " ('world', 0): 1342,\n",
       " ('lighter', 0): 26,\n",
       " ('featur', 0): 625,\n",
       " ('direct', 0): 1180,\n",
       " ('eugen', 0): 32,\n",
       " ('levi', 0): 10,\n",
       " ('he', 0): 4112,\n",
       " ('manag', 0): 536,\n",
       " ('some', 0): 3563,\n",
       " ('hi', 0): 4498,\n",
       " ('friend', 0): 1089,\n",
       " ('such', 0): 1573,\n",
       " ('as', 0): 6605,\n",
       " ('star', 0): 1238,\n",
       " ('mostli', 0): 307,\n",
       " ('engag', 0): 231,\n",
       " ('due', 0): 334,\n",
       " ('end', 0): 2475,\n",
       " ('funni', 0): 1214,\n",
       " ('littl', 0): 1966,\n",
       " ('twist', 0): 452,\n",
       " ('isnt', 0): 817,\n",
       " ('total', 0): 525,\n",
       " ('unpredict', 0): 41,\n",
       " ('but', 0): 6867,\n",
       " ('also', 0): 2924,\n",
       " ('kind', 0): 879,\n",
       " ('unexpect', 0): 142,\n",
       " ('excel', 0): 1204,\n",
       " ('expect', 0): 904,\n",
       " ('live', 0): 1545,\n",
       " ('up', 0): 3152,\n",
       " ('all', 0): 4951,\n",
       " ('hype', 0): 33,\n",
       " ('did', 0): 1689,\n",
       " ('like', 0): 4571,\n",
       " ('bourn', 0): 15,\n",
       " ('action', 0): 970,\n",
       " ('fast', 0): 222,\n",
       " ('pace', 0): 421,\n",
       " ('realist', 0): 390,\n",
       " ('intens', 0): 284,\n",
       " ('if', 0): 3709,\n",
       " ('you', 0): 4969,\n",
       " ('other', 0): 3080,\n",
       " ('two', 0): 1965,\n",
       " ('trilog', 0): 76,\n",
       " ('will', 0): 2842,\n",
       " ('love', 0): 2962,\n",
       " ('straightforward', 0): 49,\n",
       " ('no', 0): 2487,\n",
       " ('too', 0): 2092,\n",
       " ('unrealist', 0): 42,\n",
       " ('ok', 0): 176,\n",
       " ('julia', 0): 68,\n",
       " ('stile', 0): 11,\n",
       " ('show', 0): 2317,\n",
       " ('italian', 0): 181,\n",
       " ('safe', 0): 95,\n",
       " ('hous', 0): 533,\n",
       " ('farfetch', 0): 20,\n",
       " ('especi', 0): 1068,\n",
       " ('what', 0): 3601,\n",
       " ('happen', 0): 1040,\n",
       " ('supremaci', 0): 5,\n",
       " ('make', 0): 3510,\n",
       " ('sens', 0): 724,\n",
       " ('she', 0): 2102,\n",
       " ('treadston', 0): 2,\n",
       " ('know', 0): 2113,\n",
       " ('doe', 0): 1838,\n",
       " ('want', 0): 1743,\n",
       " ('him', 0): 2102,\n",
       " ('possibl', 0): 579,\n",
       " ('trust', 0): 123,\n",
       " ('person', 0): 1124,\n",
       " ('lead', 0): 939,\n",
       " ('right', 0): 1178,\n",
       " ('driven', 0): 83,\n",
       " ('their', 0): 2846,\n",
       " ('reaction', 0): 147,\n",
       " ('around', 0): 1076,\n",
       " ('thing', 0): 2042,\n",
       " ('alway', 0): 1329,\n",
       " ('about', 0): 3924,\n",
       " ('can', 0): 3258,\n",
       " ('kick', 0): 173,\n",
       " ('butt', 0): 50,\n",
       " ('match', 0): 199,\n",
       " ('with', 0): 7035,\n",
       " ('peopl', 0): 2296,\n",
       " ('fight', 0): 539,\n",
       " ('struggl', 0): 258,\n",
       " ('take', 0): 2059,\n",
       " ('damag', 0): 62,\n",
       " ('treat', 0): 282,\n",
       " ('audienc', 0): 842,\n",
       " ('idiotsal', 0): 1,\n",
       " ('actor', 0): 1827,\n",
       " ('were', 0): 2491,\n",
       " ('solid', 0): 246,\n",
       " ('perform', 0): 1965,\n",
       " ('believ', 0): 1218,\n",
       " ('damon', 0): 17,\n",
       " ('play', 0): 2510,\n",
       " ('sleep', 0): 139,\n",
       " ('receiv', 0): 226,\n",
       " ('support', 0): 619,\n",
       " ('joan', 0): 101,\n",
       " ('allen', 0): 79,\n",
       " ('repris', 0): 28,\n",
       " ('role', 0): 1401,\n",
       " ('david', 0): 350,\n",
       " ('strathairn', 0): 3,\n",
       " ('scott', 0): 151,\n",
       " ('glenn', 0): 56,\n",
       " ('recommend', 0): 1119,\n",
       " ('do', 0): 2537,\n",
       " ('miss', 0): 746,\n",
       " ('franka', 0): 5,\n",
       " ('potent', 0): 34,\n",
       " ('though', 0): 1401,\n",
       " ('review', 0): 519,\n",
       " ('dedic', 0): 71,\n",
       " ('late', 0): 506,\n",
       " ('keith', 0): 38,\n",
       " ('moon', 0): 89,\n",
       " ('entwistleth', 0): 1,\n",
       " ('origin', 0): 991,\n",
       " ('drum', 0): 35,\n",
       " ('bassther', 0): 1,\n",
       " ('seem', 0): 1724,\n",
       " ('be', 0): 5973,\n",
       " ('veri', 0): 3697,\n",
       " ('earli', 0): 586,\n",
       " ('footag', 0): 138,\n",
       " ('these', 0): 1580,\n",
       " ('more', 0): 3607,\n",
       " ('then', 0): 1894,\n",
       " ('let', 0): 817,\n",
       " ('ave', 0): 7,\n",
       " ('nowaday', 0): 67,\n",
       " ('tend', 0): 159,\n",
       " ('differ', 0): 1158,\n",
       " ('altogeth', 0): 45,\n",
       " ('parodi', 0): 93,\n",
       " ('shadow', 0): 110,\n",
       " ('much', 0): 2643,\n",
       " ('better', 0): 1503,\n",
       " ('year', 0): 2106,\n",
       " ('fair', 0): 156,\n",
       " ('prove', 0): 376,\n",
       " ('anyth', 0): 740,\n",
       " ('anyon', 0): 856,\n",
       " ('anymor', 0): 102,\n",
       " ('theyv', 0): 68,\n",
       " ('earn', 0): 93,\n",
       " ('respect', 0): 374,\n",
       " ('overtimethi', 0): 1,\n",
       " ('concert', 0): 71,\n",
       " ('me', 0): 2684,\n",
       " ('best', 0): 2463,\n",
       " ('command', 0): 123,\n",
       " ('400000', 0): 2,\n",
       " ('plu', 0): 213,\n",
       " ('strong', 0): 517,\n",
       " ('crow', 0): 21,\n",
       " ('skill', 0): 182,\n",
       " ('charisma', 0): 43,\n",
       " ('wit', 0): 268,\n",
       " ('whole', 0): 884,\n",
       " ('bloodi', 0): 87,\n",
       " ('musicw', 0): 1,\n",
       " ('act', 0): 2102,\n",
       " ('bill', 0): 249,\n",
       " ('door', 0): 153,\n",
       " ('last', 0): 1051,\n",
       " ('ever', 0): 1783,\n",
       " ('week', 0): 237,\n",
       " ('befor', 0): 1389,\n",
       " ('jim', 0): 126,\n",
       " ('morrison', 0): 6,\n",
       " ('die', 0): 575,\n",
       " ('moodi', 0): 48,\n",
       " ('blue', 0): 191,\n",
       " ('hendrix', 0): 5,\n",
       " ('tast', 0): 193,\n",
       " ('free', 0): 205,\n",
       " ('mani', 0): 2221,\n",
       " ('point', 0): 1045,\n",
       " ('whoever', 0): 52,\n",
       " ('major', 0): 412,\n",
       " ('had', 0): 2652,\n",
       " ('come', 0): 2057,\n",
       " ('see', 0): 3850,\n",
       " ('hippi', 0): 46,\n",
       " ('fest', 0): 28,\n",
       " ('1960', 0): 68,\n",
       " ('woodstock', 0): 5,\n",
       " ('record', 0): 222,\n",
       " ('out', 0): 3938,\n",
       " ('so', 0): 4151,\n",
       " ('greatest', 0): 369,\n",
       " ('work', 0): 2096,\n",
       " ('date', 0): 328,\n",
       " ('tommi', 0): 48,\n",
       " ('hungri', 0): 26,\n",
       " ('crowd', 0): 135,\n",
       " ('abl', 0): 530,\n",
       " ('experi', 0): 616,\n",
       " ('own', 0): 1312,\n",
       " ('uniqu', 0): 353,\n",
       " ('event', 0): 451,\n",
       " ('grove', 0): 3,\n",
       " ('knowledg', 0): 122,\n",
       " ('gig', 0): 18,\n",
       " ('need', 0): 1009,\n",
       " ('rock', 0): 293,\n",
       " ('n', 0): 39,\n",
       " ('roll', 0): 201,\n",
       " ('throw', 0): 196,\n",
       " ('at', 0): 4835,\n",
       " ('hungrili', 0): 2,\n",
       " ('bait', 0): 13,\n",
       " ('crowdat', 0): 1,\n",
       " ('o', 0): 63,\n",
       " ('clock', 0): 22,\n",
       " ('morn', 0): 101,\n",
       " ('august', 0): 22,\n",
       " ('1970', 0): 86,\n",
       " ('mc', 0): 7,\n",
       " ('announc', 0): 58,\n",
       " ('ladi', 0): 334,\n",
       " ('gentlemen', 0): 21,\n",
       " ('small', 0): 640,\n",
       " ('band', 0): 192,\n",
       " ('shepherd', 0): 13,\n",
       " ('bush', 0): 18,\n",
       " ('london', 0): 152,\n",
       " ('oojohn', 0): 1,\n",
       " ('entwistl', 0): 2,\n",
       " ('suit', 0): 157,\n",
       " ('black', 0): 601,\n",
       " ('leather', 0): 11,\n",
       " ('front', 0): 228,\n",
       " ('line', 0): 935,\n",
       " ('human', 0): 773,\n",
       " ('skeleton', 0): 15,\n",
       " ('neck', 0): 41,\n",
       " ('toe', 0): 21,\n",
       " ('roger', 0): 98,\n",
       " ('dress', 0): 167,\n",
       " ('tradit', 0): 232,\n",
       " ('stage', 0): 318,\n",
       " ('outfit', 0): 50,\n",
       " ('long', 0): 1118,\n",
       " ('tassel', 0): 1,\n",
       " ('flow', 0): 126,\n",
       " ('hair', 0): 154,\n",
       " ('white', 0): 400,\n",
       " ('tshirt', 0): 10,\n",
       " ('jean', 0): 135,\n",
       " ('pete', 0): 19,\n",
       " ('boiler', 0): 2,\n",
       " ('doc', 0): 29,\n",
       " ('martin', 0): 138,\n",
       " ('hed', 0): 53,\n",
       " ('prefer', 0): 140,\n",
       " ('wearth', 0): 1,\n",
       " ('stop', 0): 453,\n",
       " ('onslaught', 0): 8,\n",
       " ('high', 0): 612,\n",
       " ('energi', 0): 128,\n",
       " ('over', 0): 1608,\n",
       " ('hour', 0): 510,\n",
       " ('artist', 0): 315,\n",
       " ('track', 0): 214,\n",
       " ('young', 0): 1305,\n",
       " ('man', 0): 1637,\n",
       " ('shake', 0): 62,\n",
       " ('queue', 0): 5,\n",
       " ('shut', 0): 52,\n",
       " ('bleed', 0): 27,\n",
       " ('opera', 0): 123,\n",
       " ('went', 0): 470,\n",
       " ('wild', 0): 183,\n",
       " ('hear', 0): 338,\n",
       " ('didnt', 0): 1122,\n",
       " ('disappoint', 0): 415,\n",
       " ('straight', 0): 206,\n",
       " ('into', 0): 2476,\n",
       " ('overtur', 0): 9,\n",
       " ('air', 0): 301,\n",
       " ('final', 0): 1084,\n",
       " ('note', 0): 378,\n",
       " ('amazingto', 0): 1,\n",
       " ('captur', 0): 389,\n",
       " ('magnitud', 0): 10,\n",
       " ('statur', 0): 19,\n",
       " ('peak', 0): 67,\n",
       " ('festiv', 0): 193,\n",
       " ('anywher', 0): 110,\n",
       " ('fantast', 0): 406,\n",
       " ('piec', 0): 591,\n",
       " ('cinemat', 0): 170,\n",
       " ('historyth', 0): 5,\n",
       " ('english', 0): 343,\n",
       " ('dvd', 0): 953,\n",
       " ('soundtrack', 0): 372,\n",
       " ('englishlinear', 0): 1,\n",
       " ('pcm', 0): 1,\n",
       " ('stereo', 0): 12,\n",
       " ('state', 0): 355,\n",
       " ('51', 0): 15,\n",
       " ('least', 0): 768,\n",
       " ('check', 0): 368,\n",
       " ('local', 0): 328,\n",
       " ('press', 0): 64,\n",
       " ('details', 0): 1,\n",
       " ('okayth', 0): 1,\n",
       " ('durat', 0): 26,\n",
       " ('85', 0): 18,\n",
       " ('minut', 0): 718,\n",
       " ('extra', 0): 195,\n",
       " ('which', 0): 2962,\n",
       " ('ye', 0): 435,\n",
       " ('slice', 0): 49,\n",
       " ('histori', 0): 503,\n",
       " ('would', 0): 2879,\n",
       " ('send', 0): 158,\n",
       " ('nostalgia', 0): 54,\n",
       " ('trip', 0): 202,\n",
       " ('down', 0): 1127,\n",
       " ('memori', 0): 273,\n",
       " ('lane', 0): 57,\n",
       " ('moment', 0): 955,\n",
       " ('meant', 0): 183,\n",
       " ('raw', 0): 101,\n",
       " ('your', 0): 1842,\n",
       " ('facei', 0): 3,\n",
       " ('have', 0): 5559,\n",
       " ('given', 0): 580,\n",
       " ('ten', 0): 265,\n",
       " ('wasnt', 0): 571,\n",
       " ('lack', 0): 432,\n",
       " ('been', 0): 2382,\n",
       " ('nicethank', 0): 1,\n",
       " ('honest', 0): 194,\n",
       " ('failur', 0): 67,\n",
       " ('bad', 0): 1136,\n",
       " ('becaus', 0): 2186,\n",
       " ('realli', 0): 2774,\n",
       " ('sound', 0): 606,\n",
       " ('tri', 0): 1501,\n",
       " ('milk', 0): 27,\n",
       " ('money', 0): 433,\n",
       " ('case', 0): 484,\n",
       " ('sometim', 0): 493,\n",
       " ('odd', 0): 232,\n",
       " ('how', 0): 2299,\n",
       " ('told', 0): 454,\n",
       " ('stori', 0): 3336,\n",
       " ('timon', 0): 14,\n",
       " ('pumba', 0): 4,\n",
       " ('simba', 0): 10,\n",
       " ('troubl', 0): 303,\n",
       " ('jacuzzi', 0): 1,\n",
       " ('bubbl', 0): 30,\n",
       " ('leav', 0): 782,\n",
       " ('harmless', 0): 33,\n",
       " ('fun', 0): 976,\n",
       " ('kid', 0): 732,\n",
       " ('adult', 0): 302,\n",
       " ('video', 0): 417,\n",
       " ('trail', 0): 48,\n",
       " ('off', 0): 1393,\n",
       " ('toward', 0): 329,\n",
       " ('8', 0): 222,\n",
       " ('10', 0): 627,\n",
       " ('militari', 0): 117,\n",
       " ('train', 0): 242,\n",
       " ('becom', 0): 1060,\n",
       " ('common', 0): 199,\n",
       " ('genr', 0): 432,\n",
       " ('unto', 0): 15,\n",
       " ('themselv', 0): 436,\n",
       " ('among', 0): 360,\n",
       " ('promin', 0): 61,\n",
       " ('we', 0): 2123,\n",
       " ('offic', 0): 325,\n",
       " ('gentleman', 0): 52,\n",
       " ('top', 0): 658,\n",
       " ('gun', 0): 191,\n",
       " ('gi', 0): 9,\n",
       " ('jane', 0): 112,\n",
       " ('now', 0): 1429,\n",
       " ('men', 0): 620,\n",
       " ('honor', 0): 98,\n",
       " ('true', 0): 920,\n",
       " ('doesnt', 0): 1224,\n",
       " ('chang', 0): 748,\n",
       " ('same', 0): 1289,\n",
       " ('probabl', 0): 946,\n",
       " ('most', 0): 2750,\n",
       " ('sinc', 0): 1028,\n",
       " ('focus', 0): 216,\n",
       " ('desegreg', 0): 3,\n",
       " ('angleth', 0): 1,\n",
       " ('actual', 0): 1329,\n",
       " ('quit', 0): 1309,\n",
       " ('inspir', 0): 331,\n",
       " ('humaninterest', 0): 1,\n",
       " ('those', 0): 1592,\n",
       " ('mention', 0): 465,\n",
       " ('abov', 0): 317,\n",
       " ('carl', 0): 38,\n",
       " ('brashear', 0): 7,\n",
       " ('cuba', 0): 27,\n",
       " ('jr', 0): 106,\n",
       " ('unquestion', 0): 16,\n",
       " ('courag', 0): 95,\n",
       " ('principl', 0): 51,\n",
       " ('strength', 0): 168,\n",
       " ('shine', 0): 164,\n",
       " ('brightli', 0): 9,\n",
       " ('unfortun', 0): 354,\n",
       " ('director', 0): 1294,\n",
       " ('georg', 0): 294,\n",
       " ('tillman', 0): 3,\n",
       " ('tunnel', 0): 18,\n",
       " ('vision', 0): 128,\n",
       " ('present', 0): 572,\n",
       " ('eschew', 0): 9,\n",
       " ('develop', 0): 483,\n",
       " ('variou', 0): 231,\n",
       " ('than', 0): 2632,\n",
       " ('favor', 0): 79,\n",
       " ('constant', 0): 106,\n",
       " ('advers', 0): 19,\n",
       " ('billi', 0): 141,\n",
       " ('sunday', 0): 63,\n",
       " ('robert', 0): 430,\n",
       " ('de', 0): 221,\n",
       " ('niro', 0): 22,\n",
       " ('central', 0): 160,\n",
       " ('figur', 0): 365,\n",
       " ('except', 0): 567,\n",
       " ('initi', 0): 189,\n",
       " ('scene', 0): 2444,\n",
       " ('fistfight', 0): 7,\n",
       " ('wife', 0): 634,\n",
       " ('dont', 0): 2043,\n",
       " ('instanc', 0): 103,\n",
       " ('scar', 0): 31,\n",
       " ('palm', 0): 19,\n",
       " ('assum', 0): 125,\n",
       " ('plow', 0): 2,\n",
       " ('followup', 0): 27,\n",
       " ('mr', 0): 469,\n",
       " ('pappi', 0): 4,\n",
       " ('hal', 0): 40,\n",
       " ('holbrook', 0): 3,\n",
       " ('short', 0): 680,\n",
       " ('judg', 0): 178,\n",
       " ('rest', 0): 498,\n",
       " ('screen', 0): 943,\n",
       " ('time', 0): 3922,\n",
       " ('rant', 0): 24,\n",
       " ('characterscuba', 0): 1,\n",
       " ('give', 0): 1864,\n",
       " ('outstand', 0): 231,\n",
       " ('ive', 0): 1021,\n",
       " ('seen', 0): 2149,\n",
       " ('far', 0): 835,\n",
       " ('complet', 0): 790,\n",
       " ('part', 0): 1538,\n",
       " ('rise', 0): 136,\n",
       " ('occas', 0): 63,\n",
       " ('jerri', 0): 100,\n",
       " ('maguir', 0): 15,\n",
       " ('rod', 0): 23,\n",
       " ('tidwel', 0): 1,\n",
       " ('fascin', 0): 287,\n",
       " ('onedimension', 0): 25,\n",
       " ('depth', 0): 212,\n",
       " ('rain', 0): 94,\n",
       " ('puddl', 0): 7,\n",
       " ('complex', 0): 268,\n",
       " ('ground', 0): 132,\n",
       " ('issu', 0): 290,\n",
       " ('face', 0): 714,\n",
       " ('life', 0): 1960,\n",
       " ('crise', 0): 6,\n",
       " ('challeng', 0): 183,\n",
       " ('recoveri', 0): 7,\n",
       " ('chill', 0): 156,\n",
       " ('factor', 0): 90,\n",
       " ('dread', 0): 58,\n",
       " ('almost', 0): 1053,\n",
       " ('profession', 0): 172,\n",
       " ('suicid', 0): 126,\n",
       " ('partaft', 0): 1,\n",
       " ('stint', 0): 12,\n",
       " ('hand', 0): 631,\n",
       " ('comedian', 0): 85,\n",
       " ('analyz', 0): 34,\n",
       " ('adventur', 0): 300,\n",
       " ('rocki', 0): 39,\n",
       " ('bullwinkl', 0): 2,\n",
       " ('meet', 0): 638,\n",
       " ('parent', 0): 325,\n",
       " ('deniro', 0): 17,\n",
       " ('dramat', 0): 285,\n",
       " ('root', 0): 94,\n",
       " ('just', 0): 3632,\n",
       " ('shouldnt', 0): 110,\n",
       " ('wast', 0): 120,\n",
       " ('endow', 0): 12,\n",
       " ('hard', 0): 774,\n",
       " ('beli', 0): 8,\n",
       " ('tortur', 0): 119,\n",
       " ('soul', 0): 225,\n",
       " ('pleasur', 0): 177,\n",
       " ('watch', 0): 3314,\n",
       " ('everi', 0): 1302,\n",
       " ('charliz', 0): 7,\n",
       " ('theron', 0): 6,\n",
       " ('legend', 0): 126,\n",
       " ('bagger', 0): 1,\n",
       " ('vanc', 0): 16,\n",
       " ('yard', 0): 29,\n",
       " ('row', 0): 48,\n",
       " ('begin', 0): 969,\n",
       " ('wonder', 0): 1470,\n",
       " ('2000', 0): 54,\n",
       " ('five', 0): 236,\n",
       " ('minor', 0): 202,\n",
       " ('carri', 0): 366,\n",
       " ('stay', 0): 396,\n",
       " ('step', 0): 219,\n",
       " ('costar', 0): 68,\n",
       " ('richard', 0): 281,\n",
       " ('gere', 0): 20,\n",
       " ('cameo', 0): 143,\n",
       " ('hereth', 0): 9,\n",
       " ('interest', 0): 1525,\n",
       " ('special', 0): 687,\n",
       " ('includ', 0): 744,\n",
       " ('reflect', 0): 180,\n",
       " ('real', 0): 1546,\n",
       " ('delet', 0): 22,\n",
       " ('scenesi', 0): 4,\n",
       " ('hackney', 0): 14,\n",
       " ('rate', 0): 562,\n",
       " ('710', 0): 102,\n",
       " ('im', 0): 1163,\n",
       " ('sucker', 0): 12,\n",
       " ('underdog', 0): 17,\n",
       " ('fond', 0): 69,\n",
       " ('where', 0): 1747,\n",
       " ('theme', 0): 477,\n",
       " ('particularli', 0): 404,\n",
       " ('both', 0): 1363,\n",
       " ('area', 0): 166,\n",
       " ('bring', 0): 712,\n",
       " ('us', 0): 1269,\n",
       " ('memor', 0): 386,\n",
       " ('compens', 0): 29,\n",
       " ('shortcom', 0): 26,\n",
       " ('hari', 0): 10,\n",
       " ('om', 0): 11,\n",
       " ('imposs', 0): 195,\n",
       " ('french', 0): 297,\n",
       " ('tourist', 0): 46,\n",
       " ('autorickshaw', 0): 2,\n",
       " ('driver', 0): 89,\n",
       " ('agre', 0): 283,\n",
       " ('rendezv', 0): 6,\n",
       " ('indiffer', 0): 34,\n",
       " ('boyfriend', 0): 135,\n",
       " ('sort', 0): 500,\n",
       " ('thirdworld', 0): 2,\n",
       " ('road', 0): 183,\n",
       " ('careen', 0): 5,\n",
       " ('lush', 0): 35,\n",
       " ('reveri', 0): 3,\n",
       " ('madcap', 0): 10,\n",
       " ('distinguish', 0): 41,\n",
       " ('stellar', 0): 47,\n",
       " ('vijay', 0): 11,\n",
       " ('raaz', 0): 3,\n",
       " ('india', 0): 69,\n",
       " ('busier', 0): 1,\n",
       " ('appear', 0): 766,\n",
       " ('planner', 0): 3,\n",
       " ('mira', 0): 9,\n",
       " ('nair', 0): 13,\n",
       " ('monsoon', 0): 6,\n",
       " ('weddingin', 0): 1,\n",
       " ('interview', 0): 157,\n",
       " ('untouch', 0): 14,\n",
       " ('success', 0): 488,\n",
       " ('respond', 0): 50,\n",
       " ('care', 0): 606,\n",
       " ('pensiv', 0): 3,\n",
       " ('question', 0): 464,\n",
       " ('discov', 0): 361,\n",
       " ('join', 0): 163,\n",
       " ('theatr', 0): 124,\n",
       " ('troup', 0): 13,\n",
       " ('univers', 0): 223,\n",
       " ('formal', 0): 31,\n",
       " ('surprisingli', 0): 229,\n",
       " ('inarticul', 0): 2,\n",
       " ('craft', 0): 155,\n",
       " ('speak', 0): 321,\n",
       " ('honesti', 0): 45,\n",
       " ('puriti', 0): 14,\n",
       " ('wellspr', 0): 2,\n",
       " ('approach', 0): 213,\n",
       " ('earnest', 0): 23,\n",
       " ('desir', 0): 204,\n",
       " ('commun', 0): 195,\n",
       " ('authent', 0): 92,\n",
       " ('clear', 0): 295,\n",
       " ('convey', 0): 145,\n",
       " ('emot', 0): 691,\n",
       " ('integr', 0): 80,\n",
       " ('assur', 0): 75,\n",
       " ('lift', 0): 71,\n",
       " ('character', 0): 87,\n",
       " ('extraordinari', 0): 119,\n",
       " ('level', 0): 374,\n",
       " ('bharatbala', 0): 1,\n",
       " ('perfectli', 0): 355,\n",
       " ('express', 0): 294,\n",
       " ('camera', 0): 439,\n",
       " ('revel', 0): 75,\n",
       " ('closeup', 0): 60,\n",
       " ('compel', 0): 213,\n",
       " ('shot', 0): 765,\n",
       " ('camil', 0): 9,\n",
       " ('natta', 0): 3,\n",
       " ('gorgeou', 0): 177,\n",
       " ('frenchwoman', 0): 2,\n",
       " ('isa', 0): 5,\n",
       " ('famou', 0): 357,\n",
       " ('essay', 0): 21,\n",
       " ('wrote', 0): 178,\n",
       " ('charl', 0): 164,\n",
       " ('dicken', 0): 16,\n",
       " ('orwel', 0): 2,\n",
       " ('reader', 0): 47,\n",
       " ('regret', 0): 100,\n",
       " ('continu', 0): 407,\n",
       " ('write', 0): 494,\n",
       " ('pickwick', 0): 2,\n",
       " ('paper', 0): 58,\n",
       " ('stick', 0): 165,\n",
       " ('episod', 0): 574,\n",
       " ('novel', 0): 320,\n",
       " ('career', 0): 399,\n",
       " ('difficult', 0): 296,\n",
       " ('contemporari', 0): 125,\n",
       " ('surte', 0): 3,\n",
       " ('precis', 0): 65,\n",
       " ('concentr', 0): 73,\n",
       " ('misadventur', 0): 15,\n",
       " ('fox', 0): 94,\n",
       " ('hunt', 0): 124,\n",
       " ('set', 0): 1284,\n",
       " ('fanci', 0): 68,\n",
       " ('romford', 0): 1,\n",
       " ('hound', 0): 22,\n",
       " ('hunter', 0): 94,\n",
       " ('hors', 0): 120,\n",
       " ('lover', 0): 245,\n",
       " ('still', 0): 1940,\n",
       " ('follow', 0): 803,\n",
       " ('unread', 0): 1,\n",
       " ('determin', 0): 117,\n",
       " ('forget', 0): 321,\n",
       " ('first', 0): 2545,\n",
       " ('book', 0): 596,\n",
       " ('sketch', 0): 30,\n",
       " ('boz', 0): 2,\n",
       " ('third', 0): 246,\n",
       " ('oliv', 0): 66,\n",
       " ('got', 0): 1079,\n",
       " ('grim', 0): 68,\n",
       " ('say', 0): 1959,\n",
       " ('author', 0): 168,\n",
       " ('grow', 0): 273,\n",
       " ('style', 0): 604,\n",
       " ('definit', 0): 816,\n",
       " ('thatbut', 0): 3,\n",
       " ('overlook', 0): 95,\n",
       " ('writer', 0): 381,\n",
       " ('transcend', 0): 36,\n",
       " ('fellow', 0): 192,\n",
       " ('said', 0): 721,\n",
       " ('everyon', 0): 827,\n",
       " ('pgwodehous', 0): 1,\n",
       " ('himself', 0): 773,\n",
       " ('entertain', 0): 1020,\n",
       " ('poke', 0): 34,\n",
       " ('upper', 0): 47,\n",
       " ('reach', 0): 206,\n",
       " ('british', 0): 302,\n",
       " ('social', 0): 244,\n",
       " ('system', 0): 146,\n",
       " ('earl', 0): 32,\n",
       " ('emsworth', 0): 1,\n",
       " ('prouder', 0): 1,\n",
       " ('rais', 0): 181,\n",
       " ('finest', 0): 147,\n",
       " ('pig', 0): 32,\n",
       " ('england', 0): 113,\n",
       " ('beingwel', 0): 1,\n",
       " ('psmith', 0): 1,\n",
       " ('prepar', 0): 148,\n",
       " ('counterattack', 0): 1,\n",
       " ('suppos', 0): 331,\n",
       " ('submiss', 0): 10,\n",
       " ('unfair', 0): 36,\n",
       " ('superior', 0): 131,\n",
       " ('stanley', 0): 49,\n",
       " ('uckridg', 0): 1,\n",
       " ('perfect', 0): 859,\n",
       " ('scheme', 0): 71,\n",
       " ('should', 0): 1297,\n",
       " ('net', 0): 28,\n",
       " ('huge', 0): 361,\n",
       " ('profit', 0): 29,\n",
       " ('apart', 0): 317,\n",
       " ('jeev', 0): 1,\n",
       " ('put', 0): 1034,\n",
       " ('brilliant', 0): 595,\n",
       " ('brain', 0): 134,\n",
       " ('rescu', 0): 103,\n",
       " ('inept', 0): 31,\n",
       " ('berti', 0): 2,\n",
       " ('wooster', 0): 2,\n",
       " ('boss', 0): 141,\n",
       " ('wodehous', 0): 2,\n",
       " ('limit', 0): 204,\n",
       " ('view', 0): 858,\n",
       " ('mission', 0): 92,\n",
       " ('cartoon', 0): 196,\n",
       " ('lost', 0): 486,\n",
       " ('glow', 0): 34,\n",
       " ('serv', 0): 211,\n",
       " ('purpos', 0): 181,\n",
       " ('compar', 0): 407,\n",
       " ('seriou', 0): 349,\n",
       " ('evelyn', 0): 8,\n",
       " ('waugh', 0): 1,\n",
       " ('intellectu', 0): 89,\n",
       " ('type', 0): 462,\n",
       " ('remain', 0): 336,\n",
       " ('20th', 0): 68,\n",
       " ('centuri', 0): 240,\n",
       " ('literatur', 0): 38,\n",
       " ('brideshead', 0): 3,\n",
       " ('revisit', 0): 36,\n",
       " ('declin', 0): 30,\n",
       " ('fall', 0): 613,\n",
       " ('fail', 0): 269,\n",
       " ('frequent', 0): 104,\n",
       " ('collaps', 0): 39,\n",
       " ('vile', 0): 16,\n",
       " ('ordeal', 0): 21,\n",
       " ('gilbert', 0): 21,\n",
       " ('pinfold', 0): 1,\n",
       " ('lesser', 0): 57,\n",
       " ('hack', 0): 36,\n",
       " ('didwodehous', 0): 1,\n",
       " ('gift', 0): 100,\n",
       " ('lyricist', 0): 3,\n",
       " ('score', 0): 501,\n",
       " ('showboat', 0): 3,\n",
       " ('kern', 0): 5,\n",
       " ('hammerstein', 0): 2,\n",
       " ('tune', 0): 145,\n",
       " ('transpos', 0): 6,\n",
       " ('oh', 0): 238,\n",
       " ('dozen', 0): 88,\n",
       " ('earlier', 0): 243,\n",
       " ('handi', 0): 16,\n",
       " ('dramatist', 0): 4,\n",
       " ('pleas', 0): 356,\n",
       " ('took', 0): 410,\n",
       " ('damsel', 0): 11,\n",
       " ('distress', 0): 37,\n",
       " ('turn', 0): 1179,\n",
       " ('screenplay', 0): 218,\n",
       " ('hereit', 0): 3,\n",
       " ('normal', 0): 268,\n",
       " ('touch', 0): 637,\n",
       " ('butler', 0): 43,\n",
       " ('kegg', 0): 1,\n",
       " ('reginald', 0): 17,\n",
       " ('gardin', 0): 5,\n",
       " ('scoundrel', 0): 9,\n",
       " ('rig', 0): 12,\n",
       " ('friendli', 0): 66,\n",
       " ('gambl', 0): 28,\n",
       " ('game', 0): 360,\n",
       " ('chanc', 0): 479,\n",
       " ('staff', 0): 39,\n",
       " ('home', 0): 679,\n",
       " ('head', 0): 563,\n",
       " ('unabl', 0): 92,\n",
       " ('refrain', 0): 8,\n",
       " ('occasion', 0): 161,\n",
       " ('sing', 0): 309,\n",
       " ('constanc', 0): 9,\n",
       " ('collier', 0): 1,\n",
       " ('attempt', 0): 419,\n",
       " ('control', 0): 230,\n",
       " ('impuls', 0): 16,\n",
       " ('typic', 0): 363,\n",
       " ('way', 0): 2472,\n",
       " ('affair', 0): 145,\n",
       " ('alyc', 0): 1,\n",
       " ('keep', 0): 970,\n",
       " ('antic', 0): 65,\n",
       " ('albert', 0): 78,\n",
       " ('whom', 0): 249,\n",
       " ('win', 0): 329,\n",
       " ('pot', 0): 29,\n",
       " ('cash', 0): 54,\n",
       " ('switch', 0): 68,\n",
       " ('antagonist', 0): 24,\n",
       " ('depend', 0): 117,\n",
       " ('interestwodehous', 0): 1,\n",
       " ('lucki', 0): 115,\n",
       " ('here', 0): 1461,\n",
       " ('burn', 0): 150,\n",
       " ('allan', 0): 11,\n",
       " ('gener', 0): 640,\n",
       " ('consid', 0): 541,\n",
       " ('made', 0): 2244,\n",
       " ('togeth', 0): 854,\n",
       " ('six', 0): 121,\n",
       " ('graci', 0): 6,\n",
       " ('fred', 0): 77,\n",
       " ('astair', 0): 19,\n",
       " ('sequenc', 0): 499,\n",
       " ('song', 0): 590,\n",
       " ('stiff', 0): 31,\n",
       " ('lip', 0): 48,\n",
       " ('three', 0): 707,\n",
       " ('whisk', 0): 9,\n",
       " ('broom', 0): 4,\n",
       " ('danc', 0): 373,\n",
       " ('learn', 0): 534,\n",
       " ('marvel', 0): 190,\n",
       " ('illog', 0): 17,\n",
       " ('logic', 0): 84,\n",
       " ('use', 0): 1516,\n",
       " ('confus', 0): 266,\n",
       " ('deserv', 0): 482,\n",
       " ('keggsgardin', 0): 1,\n",
       " ('mistak', 0): 153,\n",
       " ('look', 0): 2306,\n",
       " ('either', 0): 440,\n",
       " ('stupid', 0): 197,\n",
       " ('or', 0): 3694,\n",
       " ('dialog', 0): 204,\n",
       " ('carolin', 0): 17,\n",
       " ('son', 0): 427,\n",
       " ('reggi', 0): 5,\n",
       " ('ray', 0): 140,\n",
       " ('nobl', 0): 58,\n",
       " ('leaderlead', 0): 1,\n",
       " ('imagin', 0): 512,\n",
       " ('marri', 0): 347,\n",
       " ('goodby', 0): 29,\n",
       " ('drive', 0): 258,\n",
       " ('tooth', 0): 22,\n",
       " ('supposedli', 0): 69,\n",
       " ('box', 0): 213,\n",
       " ('ginger', 0): 20,\n",
       " ('weak', 0): 220,\n",
       " ('fontain', 0): 8,\n",
       " ('remark', 0): 246,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run this cell to get an idea about the corpus of words and their occurrence along with labels. \n",
    "## In this, we are computing the frequency of occurrence of word given that a review is 'positive'.\n",
    "## Similarly, we also compute the frequence of occurence of word given that a review is 'negative'.\n",
    "freqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "759c24bc",
   "metadata": {
    "id": "759c24bc"
   },
   "source": [
    "## Q4. Training the Naive Bayes Model: (20 points)\n",
    "\n",
    "Now we are in the training phase of the Naive Bayes algorithm. In this cell, take a look at the ways to calculate the log likelihood and log prior values as these are important for testing in the next few cells. \n",
    "\n",
    "Also calculate the frequency of occurrence of words where the output is negative. In the same way, calculate the word frequency count using the above functions in order to compute the log likelihood.\n",
    "\n",
    "Return the logprior and loglikelihood output by the model from this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f280e3",
   "metadata": {
    "id": "a7f280e3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of reviews\n",
    "        train_y: a list of labels correponding to the reviews (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set(key[0] for key in freqs.keys())\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate num_pos and num_neg - the total number of positive and negative words for all documents\n",
    "    num_pos = num_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] == 1:\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            num_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            num_neg += freqs[pair]\n",
    "\n",
    "    # Calculate num_doc, the number of documents\n",
    "    num_doc = len(train_x)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents \n",
    "    pos_num_docs = (train_y==1).sum()\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents \n",
    "    neg_num_docs = (train_y==0).sum()\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(pos_num_docs) - np.log(neg_num_docs)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = find_occurrence(freqs, word, 1)\n",
    "        freq_neg = find_occurrence(freqs, word, 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1)/(num_pos + V)\n",
    "        p_w_neg = (freq_neg + 1)/(num_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n",
    "\n",
    "\n",
    "    return logprior, loglikelihood\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1561d892",
   "metadata": {
    "id": "1561d892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "88833\n"
     ]
    }
   ],
   "source": [
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, X_train, y_train)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19d9c882",
   "metadata": {
    "id": "19d9c882"
   },
   "source": [
    "### Expected Output \n",
    "\n",
    "0.0 <br>\n",
    "91425"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78b51303",
   "metadata": {
    "id": "78b51303"
   },
   "source": [
    "## Q5. Implementing Naive Bayes Predict Function: (10 points)\n",
    "\n",
    "It is now time to make our prediction as to whether a given review is negative or positive respectively. \n",
    "\n",
    "After adding the log likelihood values, ensure that the output is 1 (negative) if the sum of the log likelihood value is greater than 0 and 0 (positive) if the sum of the log likelihood is less than or equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b692c2f9",
   "metadata": {
    "id": "b692c2f9"
   },
   "outputs": [],
   "source": [
    "# TASK 4 CELL\n",
    "\n",
    "def naive_bayes_predict(review, logprior, loglikelihood):\n",
    "    '''\n",
    "    Params:\n",
    "        message: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Return:\n",
    "        total_prob: the sum of all the loglikelihoods of each word in the review (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "     # process the message to get a list of words\n",
    "    word_l = clean_review(review)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    total_prob = 0\n",
    "\n",
    "    # add the logprior\n",
    "    total_prob = total_prob + logprior\n",
    "    for word in word_l:\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood.keys():\n",
    "            # add the log likelihood of that word to the probability\n",
    "            total_prob = total_prob + loglikelihood[word]\n",
    "\n",
    "    if total_prob > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b170333",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b170333",
    "outputId": "0cf6bc90-90e8-4dee-bf95-7eaf39dce147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1\n"
     ]
    }
   ],
   "source": [
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Experiment with your own review.\n",
    "my_review = \"I thought this series was going to be another fun, action series with some dynamic plots and great performances. I was wrong. While I like Jamie Denton, this show is hardly worth watching at all, unless you enjoy watching some people brutalized and the actions of the agents supposedly warranted under the theme of national security. The show is great propaganda for the current government, and spews out jingoism as though we talk that way every day. After a couple of episodes, it was boring the hell out of me, and I started watching reruns of House Invaders on BBCAmerica instead. Rather watch CSI and Without a Trace, without a doubt.\"\n",
    "p = naive_bayes_predict(my_review, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9802c9-fafa-469c-bfc0-36d7004c5673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6242708f",
   "metadata": {
    "id": "6242708f"
   },
   "source": [
    "### Expected Output :\n",
    "The expected output is 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c4eeb71",
   "metadata": {
    "id": "7c4eeb71"
   },
   "source": [
    "## Q6. Implementing Naive Bayes Test function: (10 points)\n",
    "\n",
    "In this function, implement the previous functions such as naive_bayes_predict to get the predictions for the test set. \n",
    "\n",
    "In addition to this, the function should return the total number of reviews that it correctly classified as 'positive' or 'negative'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a511e7",
   "metadata": {
    "id": "66a511e7"
   },
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of reviewa\n",
    "        test_y: the corresponding labels for the list of reviews\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of reviews classified correctly)/(total # of reviews)\n",
    "    \"\"\"\n",
    "    accuracy = 0  \n",
    "\n",
    "    \n",
    "    y_hats = []\n",
    "    error = 0\n",
    "    error_avg = 0\n",
    "    for reviews in test_x:\n",
    "        # if the prediction is > 0\n",
    "        pred = naive_bayes_predict(reviews, logprior, loglikelihood)\n",
    "        if pred > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "    test_y = test_y.values.tolist()    \n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    for i in range(len(test_x)):\n",
    "        error = error + abs(y_hats[i] - test_y[i])\n",
    "    error_avg = error/len(test_y)\n",
    "\n",
    "    accuracy = (len(test_x) - error)/len(test_x)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a9c5d9d",
   "metadata": {
    "id": "8a9c5d9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you like original gut wrenching laughter you will like this movie. If you are young or old then y -> 0.00\n",
      "What a waste of talent. A very poor, semi-coherent, script cripples this film. Rather unimaginative  -> 1.00\n",
      "I have seen this film at least 100 times and I am still excited by it, the acting is perfect and the -> 0.00\n",
      "Cheap, amateurish, unimaginative, exploitative... but don't think it'll have redeeming amusement val -> 1.00\n"
     ]
    }
   ],
   "source": [
    "# For grading purpose only\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Run this cell to test your function\n",
    "\n",
    "for review in [\"If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.<br /><br />Great Camp!!!\",\n",
    "                \"What a waste of talent. A very poor, semi-coherent, script cripples this film. Rather unimaginative direction, too. Some VERY faint echoes of Fargo here, but it just doesn't come off.\",\n",
    "                \"I have seen this film at least 100 times and I am still excited by it, the acting is perfect and the romance between Joe and Jean keeps me on the edge of my seat, plus I still think Bryan Brown is the tops. Brilliant Film.\",\n",
    "                \"Cheap, amateurish, unimaginative, exploitative... but don't think it'll have redeeming amusement value. About as unentertaining, uninstructive and just plain dull as a film can be.\"]:\n",
    "    p = naive_bayes_predict(review, logprior, loglikelihood)\n",
    "    print(f'{review[:100]} -> {p:.2f}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e2ef98",
   "metadata": {
    "id": "43e2ef98"
   },
   "source": [
    "### Expected Output :\n",
    "\n",
    "If you like original gut wrenching laughter you will like this movie. If you are young or old then y -> 0.00 <br>\n",
    "What a waste of talent. A very poor, semi-coherent, script cripples this film. Rather unimaginative  -> 1.00<br>\n",
    "I have seen this film at least 100 times and I am still excited by it, the acting is perfect and the -> 0.00 <br>\n",
    "Cheap, amateurish, unimaginative, exploitative... but don't think it'll have redeeming amusement val -> 1.00\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "216fa97a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "216fa97a",
    "outputId": "9d1f21c7-b324-43c2-e841-269c0306cbb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own review below\n",
    "my_review = 'The moview was very boring, I wanted to leave in the middle'\n",
    "naive_bayes_predict(my_review, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92517269-f3c7-4380-b853-5018b3fb2746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "216fa97a",
    "outputId": "9d1f21c7-b324-43c2-e841-269c0306cbb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own review below\n",
    "my_review ='If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie,'\n",
    "naive_bayes_predict(my_review, logprior, loglikelihood)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a45e4f0",
   "metadata": {
    "id": "8a45e4f0"
   },
   "source": [
    "### Expected Output :\n",
    "1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "mAIkM4aCC1H7",
   "metadata": {
    "id": "mAIkM4aCC1H7"
   },
   "source": [
    "# Q7. Evaluate the accuracy (10 Points)\n",
    "1. Split your data into training and test sets using random selection. Set the seed as parameter of the function so that user can select a different training and test set by changin seed.\n",
    "\n",
    "2. Calculate model paramters with training set.\n",
    "\n",
    "3. Print confusion matrix for training and test set.\n",
    "\n",
    "4. Examine False Positive and False Negative cases and provide reasoning why they get misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce02f38-c0b9-4794-ab33-4f5c32920fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9293770605587368, Test accuracy: 0.8499325236167341\n",
      "Test set confusion matrix:\n",
      "[[3096  605]\n",
      " [ 512 3197]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def split_data(df, seed):\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.3, random_state=seed)\n",
    "\n",
    "    # mapping positive and negative to 0 and 1\n",
    "    output_map = {'positive': 0, 'negative': 1}\n",
    "    y_train = y_train.map(output_map)\n",
    "    y_test = y_test.map(output_map)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(df, 120)\n",
    "\n",
    "freqs = review_counter({}, x_train, y_train)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, x_train, y_train)\n",
    "\n",
    "# Save model parameters\n",
    "pickle.dump((logprior, loglikelihood), open('naive_params.pkl', 'wb'))\n",
    "\n",
    "# Test the model on the training and test sets\n",
    "train_accuracy = test_naive_bayes(x_train, y_train, logprior, loglikelihood)\n",
    "test_accuracy = test_naive_bayes(x_test, y_test, logprior, loglikelihood)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy}, Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Clean the test reviews\n",
    "cleaned_reviews = [' '.join(clean_review(review)) for review in x_test]\n",
    "\n",
    "# Get the predicted sentiments for the test set\n",
    "y_pred_test = [naive_bayes_predict(review, logprior, loglikelihood) for review in cleaned_reviews]\n",
    "\n",
    "# Compute and print the confusion matrix for the test set\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Test set confusion matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d10755df-b0f1-4ea5-a559-385921da37e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some false positives:\n",
      "9101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I loved this movie. It's a lot of laughs. The acting is good and the writing is really sharp. I'd rather see a hundred movies like this than THREE LORD OF THE RINGS repeating and repeating themselves.<br /><br />It's a low budget affair and seems to be shot on DV but looks good and Jay Mohr and Julianne Nicholson are great together. Why do you have a ten line minimum? I'm not a critic, just a patron.<br /><br />I doubt very much that Quentin Tarantino could write a picture this funny without filling it with masturbatory gratuitous violence. This movie should be seen on more screens than just one. I laughed from beginning to end. >\n",
      "23972    This film was quite a surprise. I wasn't expecting much, to be honest. Greta Garbo's first Hollywood film? So what? Probably something rough and with the usual exaggerated arm-waving and facial contortions that low-grade silent films so often show.<br /><br />Well, was I mistaken. Greta Garbo must have just shocked the studio people as much as she did me, because this film made her a star, and deservedly so. She instinctively understood the power of just standing still, or of simply holding a meaningful expression for a long, lingering moment so its effect could be felt and not just seen. I kept thinking to myself, how did this modern actress get into a creaky silent film? She was just years and years ahead of her time.<br /><br />The story isn't all that interesting, it is the usual tale of love found and lost. It is only the performance of Garbo as Leonora, the poor village girl who makes good in the big city and then returns to get the man who got away, that gives it life. Co-star Ricardo Cortez is serviceable, but his character is never really developed and he is demeaned by the script throughout. At one point he is made to wear Garbo's clothes, leave in a huff, then ignominiously return and give them back. And let's not even talk about his horrible final scenes with Garbo and then his wife. Way too ordinary, he didn't have much of a career after this and I can understand why. But he suffices as the somewhat mystifying object of Leonora's obsession.<br /><br />There is one utterly fantastic line in this film that just says it all. \"Leonora, you are becoming conspicuous.\" Yes, conspicuous indeed.<br /><br />If you really want to know what it's all about without seeing the whole film, just watch the last 30 seconds or so - Garbo's glance as she sits quite alone in her luxurious car says everything. \"She must be so happy, she has everything she wants\" - yes, Garbo's face says it all about that. I have to see this one again, Garbo is just amazing.\n",
      "22827                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Though this movie is cheesiness at its best, it is pulled off perfectly. This movie, without a doubt, has to be considered a modern classic. There are basically two kinds of movies I like - movies with depth (chick flicks, if you must - I blame my wife for this) and mindless comedies where I can sit back and relax. This movie is a perfect example of the latter.<br /><br />A friend of mine turned me on to this movie shortly after its release. Considering me to somewhat shallow, he said to me, \"You've got to see this movie. It's just your type of movie.\" Foregoing the insult, I started watching. I know they mentioned The Ramones a million times, but when you actually see them, I said, \"Hey, it is The Ramones.\" My friend replied, \"I don't know they were a real band.\" I had my moment of glory.<br /><br />This movie, though now somewhat dated, is a must see for Ramones fans - or anyone else for that matter.\n",
      "1421                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        I must say that I really had no idea that I was going to sit down and watch this movie. I guess it was the fact that I had nothing better to do between class. But, for once a TV movie caught my interest. More importantly Helen Hunt caught my eye. I really wasn't a big fan of hers prior to this film. Sure I liked Twister and As Good As It Gets. But, something about this movie really did it for me. I would now see myself as a huge fan. This movie comes with high marks from...me. Give it a chance, it won't let you down.\n",
      "18078                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     My Super Ex Girlfriend turned out to be a pleasant surprise for me, I was really expecting a horrible movie that would probably be stupid and predictable, and you know what? It was! But this movie did have so many wonderful laughs and a fun plot that anyone could get a kick out of. I know that this was a very cheesy movie, but Uma and Anna were just so cool and Steve was such a great addition along with a great cast that looked like they had so much fun and that's what made the movie really work.<br /><br />Jenny Johnson(scary, that's my best friend's actual name) is not your typical average librarian looking woman, when Matt, your average male, asks her out, he's in for more than he expected, he's asked G-Girl out on a date, the super hero of the world! But when he finds out what a jealous and crazy girl she really is and decides that it may be a good idea that they spend some time apart, but Jenny won't have it since he's fallen for another girl, Hannah, and she will make his life a living hell, I mean, let's face it, he couldn't have chosen a better girl to break up with.<br /><br />The effect were corny, but you seriously move past them quickly, the story and cast made the story really work and I loved Uma in this movie, it was such a step up from Prime. My Super Ex Girlfriend is a fun movie that you shouldn't really take seriously, it's just a cute romantic comedy that I think if I could get a laugh out of it, anyone could.<br /><br />7/10\n",
      "Name: review, dtype: object\n",
      "\n",
      "Some false negatives:\n",
      "10850                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman, becomes a curious project to come out of gritty Warner Bros. Paul Lukas, reprising his Broadway role and winning the Best Actor Oscar, plays an anti-Nazi German underground leader fighting the Fascists, dragging his American wife and three children all over Europe before finding refuge in the States (via the Mexico border). They settle in Washington with the wife's wealthy mother and brother, though a boarder residing in the manor is immediately suspicious of the newcomers and spends an awful lot of time down at the German Embassy playing poker. It seems to take forever for this drama to find its focus, and when we realize what the heart of the material is (the wise, honest, direct refugees teaching the clueless, head-in-the-sand Americans how the world has suddenly changed), it seems a little patronizing--the viewer is quite literally put in the relatives' place, being lectured to. Lukas has several speeches in the third-act which undoubtedly won him the Academy Award, yet for the much of the picture he seems to do little but enter and exit, enter and exit. As his spouse, Bette Davis enunciates like nobody else and works her wide eyes to good advantage, but the role doesn't allow her much color. Their children (all with divergent accents!) are alternately humorous and annoying, and Geraldine Fitzgerald has a nothing role as a put-upon wife (and the disgruntled texture she brings to the part seems entirely wrong). The intent here was to tastefully, tactfully show us just because a (WWII-era) man may be German, that doesn't make him a Nazi sympathizer. We get that in the first few minutes; the rest of this tasteful, tactful movie is made up of exposition, defensive confrontation and, ultimately, compassion. It should be a heady mix, but instead it's rather dry-eyed and inert. ** from ****\n",
      "24110                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            It's another variation on the oft-told tale of two people getting married and having to share their brood of kids. WITH SIX YOU GET EGG ROLL is directed by Howard Morris (from television) and it shows, because it's the kind of tale that plays like a half-hour situation comedy padded out to feature film length--but with a scarcity of laughs, or to put it differently, only the number of laughs that would have been possible within the half-hour limits of a TV show.<br /><br />DORIS DAY decided to call it quits after this film--and it's rather easy to see why. Even the presence of some fairly reliable actors in the cast doesn't help. BRIAN KEITH, BARBARA HERSHEY, PAT CARROLL and ALICE GHOSTLEY do their best, but the script is the real problem and should have been left untouched for the big screen.<br /><br />Nothing much can be said in favor of it. Skip it and see Miss Day in any number of her more worthwhile films.\n",
      "24321                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"A research scientist is experimenting with human DNA in an attempt to create the perfect human being. His work has made it to the point where he can take a human fetus and accelerate its growth to that of an adult within a few days. His latest creation is a (spoiler omitted), but side effects from the process (spoiler omitted),\" according to the DVD sleeve's synopsis.<br /><br />\"Embryo\" opens by promising: \"The film you are about to see is not all science fiction. It is based upon medical technology which currently exists for fetal growth outside the womb. It could be a possibility tomorrow\n",
      " or today,\" according to Dr. Charles M. Brinkman III. Right. And, Dr. Joyce Brothers appears, later, at a party with Roddy McDowall.<br /><br />First, we see Rock Hudson (as Dr. Paul Holliston) light a cigarette and drive recklessly (watch that speedometer!) during a storm; unfortunately, he hits a dog. Mr. Hudson takes the wounded canine home. He learns it is pregnant, and manages to save the life of one of the puppies, due to his experimental knowledge of fetal growth. What this really boils down to is that Hudson uses an experimental drug to grow the embryo, so that it can survive outside the mother's womb. The dog, \"Number One\", grows to adult-size rapidly, and is passed off as its mother.<br /><br />Hudson lives with his sister-in-law Diane Ladd (as Martha Douglas); since his wife Nicole, also a doctor, died in another car accident. Ms. Ladd seems more emotionally stable about Nicole's death than Hudson, who survived the crash that killed his wife. Things begin to get creepy when Hudson's dog shows an intelligence level far above any normal dog. Then, Hudson decides to use his accelerated embryo growth on a human, Barbara Carrera (as Victoria Spencer).<br /><br />Hudson and the cast try their best; but, the \"Embryo\" storyline is wretchedly absurd nonsense. If you take away her silly opening and closing scenes, Ms. Carrera's valiant characterization almost works; she might have been a bigger star, if offered better films than this. The infantile ending suggests a sequel; but, happily, the idea was aborted.\n",
      "10437    The discussion has been held a thousand times. Is the \"Merchant of Venice\" antisemitic? (I think it is.) Isn't it unfair to always point out this little bit of antisemitism in an otherwise great piece of art? (I think it isn't.) Does this play stain Shakespeare's reputation as the world's greatest playwright? (I think it does.) Does it play a role if he didn't do it on a particular racist purpose? (I think it doesn't.) Michael Radford knew all this and this is why he added to his movie a prologue about the pitiful situation of the Jews in Renaissance Venice.<br /><br />In vain; for the play remains what it has always been and the new make-up only gives a first (but futile) hope that someone has dared to set something right that remains a permanent outrage, not because its degree of antisemitism would be particularly shocking but because the play comes under the name of William Shakespeare.<br /><br />Why spend so much time in portraying the hatred of a man -- Shylock? Why employ a great and serious actor like Al Pacino, if in the end everything is getting ruined in this outrageous (but hey, I'm-not-responsible-Shakespeare-wrote-it) court room scene. And now I'd like to be very precise, just like Shylock himself.<br /><br />He's demanding his right, according to the contract which the -- not very responsible -- Christian Antonio, who always used to look down on him, signed in full awareness of the consequences. Sure, what Shylock demands is cruel and useless, but that's not the point. What we see (or should see) is a man who has been humiliated for all his life, to the point where all what remains on him is his hatred. I think, it is certainly a bit inappropriate to lecture such a man on things like compassion.<br /><br />But what the play/the movie (they are one and the same now) does at this point is... become a soap opera! The cruel madman with his knife, the horrified (but rather short-minded) audience, the poor \"victim\" tied to his chair. True, Antonio accepts his fate but why can't he just say one word, \"sorry\"? I think we need not lose many words on the ridiculous verdict of the young Dottore from Padua; it's a truly \"popular verdict\" not much different from what would be seen 400 years later in the show trials of the Nazis. From one minute to the next this Jew is robbed of everything he owned, sentenced to being baptized Christian, and kicked out.<br /><br />Isn't that outrageous??? Obviously not. The story moves on to the romantic intricacies of the rings and its happy end.<br /><br />What one can learn in Libeskind's Jewish Museum in Berlin and similar places all over the world is that antisemitism often goes unnoticed by the mass because what's so devastating for a minority or some individuals is embedded in the alleged greater good for the majority. It should be exactly the task of everyone of us to develop a sensitivity to detect and unmask such tendencies.<br /><br />I don't accept the excuse that this film was made to create empathy with the badly treated Shylock (it just doesn't work out). I don't think that anybody can be forced to be merciful.<br /><br />I don't recommend this movie; in particular not for an Oscar.\n",
      "6218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Once again the same familiar story about a man (writer here) who sell his soul to the devil in order to have his most desired ambition in life: success. Unfunny script (we should \"go home and write better\"), ridiculous lines in order to understand the \"strong\" \"Christmanish\" message (our only aspiration in life is to find love, respect and a good friendship) and a very long trial scene at the end where the agent Hopkins beat the devil (Jennifer Love Hewitt is no sexy or evil at all) for all the bad things she made to this unlikable character. Not bad efforts from the actors (Baldwin also as a director, Cattrall in a \"Sex and the City\" role again, Aykroyd with some funny lines in his limited role). P.S. Try also a not so popular film from Greece called \"Alloimono stous neous\", a brilliant adaptation of this myth (an old man give his soul to the devil to get back his youth)\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame({\n",
    "        'review': x_test,\n",
    "        'actual_sentiment': y_test,\n",
    "        'predicted_sentiment': y_pred_test})\n",
    "\n",
    "# Get the false positives (reviews that were negative but predicted as positive)\n",
    "false_positives = df_test[(df_test['actual_sentiment'] == 0) & (df_test['predicted_sentiment'] == 1)]\n",
    "\n",
    "# Get the false negatives (reviews that were positive but predicted as negative)\n",
    "false_negatives = df_test[(df_test['actual_sentiment'] == 1) & (df_test['predicted_sentiment'] == 0)]\n",
    "\n",
    "# adjusting column width to see review\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print some of the misclassified reviews\n",
    "print(\"Some false positives:\")\n",
    "print(false_positives.sample(5)['review'])\n",
    "\n",
    "print(\"\\nSome false negatives:\")\n",
    "print(false_negatives.sample(5)['review'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "XbzttYVnBo7W",
   "metadata": {
    "id": "XbzttYVnBo7W"
   },
   "source": [
    "# Q8. Modularize your calssifier (10 points)\n",
    "1. Convert your code into a python module text_classifier.py\n",
    "\n",
    "2. The user should be able to launch the application on command prompt using python test_classifier.py command. The module will automatically load the model paramters from a local file of your choice and be ready to take the input from user on command prompt. The program will preprocess user input, tokenize and predict the class.\n",
    "\n",
    "3. Your module will take the input from user and output sentiment class in an indefinite loop. The output should printout the probabilities for each input token along with the final classification decision. Program will quit if user enters X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db48743f-0e7a-4f4f-ba4c-ac83acaefa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review ('X' to quit):  Worse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review ('X' to quit):  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review ('X' to quit):  nice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review ('X' to quit):  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review ('X' to quit):  x\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#import text_classifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Load the model parameters from a local file\n",
    "def load_model_parameters(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        logprior, loglikelihood = pickle.load(f)\n",
    "    return logprior, loglikelihood\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the model parameters\n",
    "    model_file = 'naive_params.pkl'  \n",
    "    logprior, loglikelihood = load_model_parameters(model_file)\n",
    "\n",
    "    # Start the indefinite loop\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Enter a review ('X' to quit): \")\n",
    "\n",
    "        # Check for exit condition\n",
    "        if user_input.lower() == 'x':\n",
    "            break\n",
    "\n",
    "        # Preprocess the input\n",
    "        cleaned_reviews = ' '.join(clean_review(user_input))\n",
    "        \n",
    "\n",
    "        # Predict the sentiment class\n",
    "        sentiment = naive_bayes_predict(cleaned_reviews, logprior, loglikelihood)\n",
    "\n",
    "        # Output the sentiment class\n",
    "        print(\"Sentiment:\", sentiment)\n",
    "        print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82e6e4d1",
   "metadata": {
    "id": "82e6e4d1"
   },
   "source": [
    "# Q9. Theory Questions: (10 points)\n",
    "\n",
    "1. Why is Laplace Smoothing or Additive Smoothing required while executing Naive Bayes operations, especially for text classification? Show how not having additive smoothing leads to bad outcomes by using an example of training and the test set. (10 points)\n",
    "\n",
    "\n",
    "2. Why are logarithmic values computed instead of only probability values in the Naive Bayes algorithm? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6339228f-4e12-4248-a202-a2df66ba1cf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 69) (495763388.py, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 69\u001b[0;36m\u001b[0m\n\u001b[0;31m    Let's consider a test review: \"The movie was fun.\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 69)\n"
     ]
    }
   ],
   "source": [
    "1. Solution:\n",
    "In the context of Naive Bayes operations, particularly for text classification,\n",
    "Laplace Smoothing, also referred to as Additive Smoothing, is a widely used technique. \n",
    "Its purpose is to handle the problem of zero probabilities and mitigate the risk of \n",
    "overconfident predictions that may arise due to the absence of specific features.\n",
    "\n",
    "To understand the concept of Additive Smoothing/ Laplace smoothing, lets consider an example of a movie review.\n",
    "And lets classify the reviews based on their words to whether its a positive review or a negative review.\n",
    "Consider the training set with two reviews.\n",
    "\n",
    "review 1 : \"The movie was amazing and highly entertaining\"\n",
    "review 2 : \"The movie was horrible and boring.\"\n",
    "\n",
    "Before going into the actual operation of Naive Bayes, first tokenize them to words to create a vocabulary.\n",
    "\n",
    "Vocabulary: [\"the\", \"movie\", \"was\", \"amazing\", \"highly\", \"entertaining\", \"horrible\", \"boring\"]\n",
    "\n",
    "With these words, we calculate the frequency of these words under positive and negative.\n",
    "\n",
    "Positive Review Frequency: {\"the\": 1, \"movie\": 1, \"was\": 1, \"amazing\": 1, \"highly\": 1, \"entertaining\": 1}\n",
    "Negative Review Frequency: {\"the\": 1, \"movie\": 1, \"was\": 1, \"horrible\": 1, \"boring\": 1}\n",
    "\n",
    "Lets consider a test review as. \"The movie was fun\"\n",
    "\n",
    "Case 1 : Naive Bayes without Laplace Smoothing\n",
    "\n",
    "Using Naive Bayes without Laplace Smoothing, we calculate the probabilities for each \n",
    "class (positive and negative)based on the word frequencies in the training set.\n",
    "\n",
    "P(positive) = 1/2\n",
    "P(negative) = 1/2\n",
    "\n",
    "P(\"the\"/positive) = 1/6\n",
    "P(\"movie\"/positive) = 1/6\n",
    "P(\"was\"/positive) = 1/6\n",
    "P(\"fun\"/positive) = 0 # Word not present in positive review frequency\n",
    "P(\"the\"/negative) = 1/6\n",
    "P(\"movie\"/negative) = 1/6\n",
    "P(\"was\"/negative) = 1/6\n",
    "P(\"fun\"/negative) = 0 # Word not present in negative review frequency\n",
    "\n",
    "\n",
    "To predict the class of the test review, we calculate the log likelihoods and sum them up:\n",
    "\n",
    "Log Likelihood for Positive Class = log(P(positive)) + log(P(\"the\"/positive)) + log(P(\"movie\"/positive)) + log(P(\"was\"/positive)) + log(P(\"fun\"/positive))\n",
    "= log(1/2) + log(1/6) + log(1/6) + log(1/6) + log(0)\n",
    "= Undefined (Log of zero is undefined)\n",
    "\n",
    "Due to the absence of the word 'fun' in the positive review frequency, its probability becomes zero, \n",
    "which results in an undefined log likelihood. Consequently, the model fails to make a prediction.\n",
    "\n",
    "This example demonstrates the issue of encountering zero probabilities and emphasizes the necessity of Laplace Smoothing. \n",
    "By applying Laplace Smoothing, a small constant (typically 1) is added to the numerator, and a multiple of that constant is \n",
    "added to the denominator during probability calculations. This ensures that even if a word is not present in the training set, \n",
    "it still receives a non-zero probability estimate. Consequently, the model becomes capable of making predictions\n",
    "\n",
    "Case 2 : Naive Bayes with Laplace Smoothing\n",
    "\n",
    "Suppose we have the following training set:\n",
    "\n",
    "Positive Review: \"The movie was fun and entertaining.\"\n",
    "Negative Review: \"The movie was boring and uninteresting.\"\n",
    "\n",
    "Using Laplace Smoothing, we calculate the word frequencies in the positive and negative reviews:\n",
    "\n",
    "Positive Review Frequency: {\"the\": 1, \"movie\": 1, \"was\": 1, \"fun\": 1, \"and\": 1, \"entertaining\": 1}\n",
    "Negative Review Frequency: {\"the\": 1, \"movie\": 1, \"was\": 1, \"boring\": 1, \"and\": 1, \"uninteresting\": 1}\n",
    "\n",
    "Let's consider a test review: \"The movie was fun.\"\n",
    "\n",
    "Using Laplace Smoothing:\n",
    "\n",
    "P(positive) = 1/2\n",
    "P(negative) = 1/2\n",
    "\n",
    "P(\"the\"/positive) = 1/7\n",
    "P(\"movie\"/positive) = 1/7\n",
    "P(\"was\"/positive) = 1/7\n",
    "P(\"fun\"/positive) = 1/(6 + 8) = 1/14 (Laplace Smoothing)\n",
    "P(\"the\"/negative) = 1/7\n",
    "P(\"movie\"/negative) = 1/7\n",
    "P(\"was\"/negative) = 1/7\n",
    "P(\"fun\"/negative) = 1/(6 + 8) = 1/14 (Laplace Smoothing)\n",
    "\n",
    "Calculating the log likelihoods and summing them up:\n",
    "\n",
    "Log Likelihood for Positive Class = log(P(positive)) + log(P(\"the\"/positive)) + log(P(\"movie\"/positive)) + log(P(\"was\"/positive)) + log(P(\"fun\"/positive))\n",
    "= log(1/2) + log(1/7) + log(1/7) + log(1/7) + log(1/14)\n",
    "= -1.79176\n",
    "\n",
    "Log Likelihood for Negative Class = log(P(negative)) + log(P(\"the\"/negative)) + log(P(\"movie\"/negative)) + log(P(\"was\"/negative)) + log(P(\"fun\"/negative))\n",
    "= log(1/2) + log(1/7) + log(1/7) + log(1/7) + log(1/14)\n",
    "= -1.79176\n",
    "\n",
    "Since the log likelihood for the positive class (-1.79176) is greater than the log likelihood for the negative class (-1.79176), \n",
    "we predict the test review as positive.\n",
    "\n",
    "Leveraging Laplace Smoothing in this scenario enables us to assign non-zero probabilities to the word \n",
    "'fun' in both the positive and negative classes, even if it was absent in the training data. By doing so,\n",
    "we ensure that all features contribute to the prediction process and eliminate the issue of encountering \n",
    "zero probabilities, leading to more accurate predictions.\n",
    "\n",
    "The utilization of Laplace Smoothing addresses the challenge of handling unseen words and prevents the model \n",
    "from becoming excessively confident or encountering undefined log likelihoods. It enhances the overall resilience \n",
    "and dependability of the Naive Bayes classifier, particularly in text classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8353d25-8a34-44d7-82b7-13e04662736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) \n",
    "Solution :\n",
    "We use Logarithm values in Naive Bayes algorithm because:\n",
    "   \n",
    "In order to avoid multiplication with lower probabilities, we use log methodology. When calculating the probability for an even happening,\n",
    "we have to multiply all the probabilities of all the word occurences in the training and vocabulary set, \n",
    "the product starts becoming less than zero which becomes zero. Therefore, we apply the log probability methodology, in order to avoid the underflow problem. \n",
    "As defined above formulation for niave bayes:\n",
    "\n",
    "    y_pred = argmax P(y)*P(x_1/y)*P(x_2/y)...P(x_n/y)**\n",
    "    \n",
    "When you implement the calculation of probabilities in code, a problem arises when the vocabulary size (m) becomes very large.\n",
    "This is because each probability term falls between 0 and 1, and when you multiply many of these probabilities together, \n",
    "the overall product approaches zero rapidly. This becomes more significant as more features are added. For example, \n",
    "with 1,000 features, multiplying 1,000 probabilities together results in an extremely tiny number.\n",
    "\n",
    "To overcome this issue, a common approach is to work with log-probabilities instead. \n",
    "Instead of using the probabilities directly, we take the logarithm (base 10 or natural logarithm) of the probabilities. \n",
    "By doing this, we can transform the multiplicative calculations into additive calculations. \n",
    "The logarithm of a product is equal to the sum of the logarithms of the individual terms.\n",
    "\n",
    "Applying logarithms to probabilities increases the computational efficiency of the naive Bayes algorithm\n",
    "and reduces the likelihood of encountering extremely small probabilities. By working with log-probabilities,\n",
    "we can avoid numerical underflow issues and make the probability calculations more manageable.\n",
    "\n",
    "In summary, the use of log-probabilities is a practical solution to handle large vocabulary sizes \n",
    "and maintain numerical stability in the naive Bayes algorithm."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS6120_NLP_Assignment_1_Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
